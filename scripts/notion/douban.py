import argparse
from calendar import month
from cmath import pi
from datetime import date, datetime
import csv
from nis import match
from os import stat
import re
import time
import feedparser
import notion_api
from notion_api import Page
from notion_api import Properties
from notion_api import Children
from notion_api import DatabaseParent
from bs4 import BeautifulSoup
from http.cookies import SimpleCookie
import requests
from requests.utils import cookiejar_from_dict
import requests
from config import(
    BOOK_DATABASE_ID,
    MOVIE_DATABASE_ID,
)


def parse_cookie_string(cookie_string):
    cookie = SimpleCookie()
    cookie.load(cookie_string)
    cookies_dict = {}
    cookiejar = None
    for key, morsel in cookie.items():
        cookies_dict[key] = morsel.value
        cookiejar = cookiejar_from_dict(
            cookies_dict, cookiejar=None, overwrite=True
        )
    return cookiejar

url = 'https://www.douban.com/feed/people/malinkang/interests'
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'}

WEREAD_BASE_URL = "https://weread.qq.com/"
rating_dict = {
    'å¾ˆå·®': 'â­ï¸',
    'è¾ƒå·®': 'â­ï¸â­ï¸',
    'è¿˜è¡Œ': 'â­ï¸â­ï¸â­ï¸',
    'æ¨è': 'â­ï¸â­ï¸â­ï¸â­ï¸',
    'åŠ›è': 'â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸',
}
rating_dict2 = {
    '': 'â­ï¸',
    '1': 'â­ï¸',
    '2': 'â­ï¸â­ï¸',
    '3': 'â­ï¸â­ï¸â­ï¸',
    '4': 'â­ï¸â­ï¸â­ï¸â­ï¸',
    '5': 'â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸',
}

def feed_parser():
    d = feedparser.parse(url)
    for entry in d.entries:
        title = entry['title']
        pattern = r'æƒ³çœ‹|åœ¨çœ‹|çœ‹è¿‡|æƒ³è¯»|æœ€è¿‘åœ¨è¯»|è¯»è¿‡'
        status = ""
        m = re.match(pattern, title)
        if m:
            status = m.group(0)
            if(status == 'æœ€è¿‘åœ¨è¯»'):
                status = status[2:]
        link = entry['link']
        if 'https' not in link:
            link = link.replace('http','https')
        rating = ''
        note = ''
        date = datetime(*entry.published_parsed[:6])
        soup = BeautifulSoup(entry['description'])
        
        for p in soup.find_all('p'):
            if 'æ¨è: ' in p.string:
                rating = rating_dict[p.string.split(": ")[1]]
            if 'å¤‡æ³¨: ' in p.string:
                note = p.string.split(": ")[1]
        if ('çœ‹' in status):
            parse_movie(date, rating, note, status, link)
        elif ('è¯»' in status):
            parse_book(date, rating, note, status, link)

def parse_movie_csv():
    with open('./data/db-movie-20220918.csv', newline='') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            title = row['\ufeffæ ‡é¢˜']
            print(title)
            status='çœ‹è¿‡'
            date = datetime.strptime(row['æ‰“åˆ†æ—¥æœŸ'],'%Y/%m/%d')
            print(date)
            rating = rating_dict2[row['ä¸ªäººè¯„åˆ†']]
            note = row['æˆ‘çš„çŸ­è¯„']
            link =row['æ¡ç›®é“¾æ¥'].strip()
            time.sleep(2)
            parse_movie(date, rating, note, status, link)


def parse_movie(date, rating, note, status, link):
    f = {"property": "æ¡ç›®é“¾æ¥", "url": {"equals": link}}
    response = notion_api.query_database(
        database_id=MOVIE_DATABASE_ID, filter=f)
    if (len(response['results']) > 0):
        update(date, rating, note, status,response['results'][0]['id'])
        return
    response = requests.get(link, headers=headers)
    soup = BeautifulSoup(response.content)
    title = soup.find(property='v:itemreviewed').string
    year = soup.find('span', {'class': 'year'}).string[1:-1]
    info = soup.find(id='info')
    # print('info ',info)
    cover = soup.find(id='mainpic').img['src']
    # å¯¼æ¼”
    directors = list(filter(lambda x: '/' not in x,info.find('span', {'class': 'attrs'}).strings))
    # æ¼”å‘˜
    actors = list()
    actor_span=info.find('span', {'class': 'actor'})
    if actor_span!=None:
        actors = list(map(lambda x: x.string,actor_span.find_all('a')))
    # ç±»å‹
    genre = list(map(lambda x: x.string, info.find_all(property='v:genre')))
    country = ''
    imdb = ''
    for span in info.find_all('span', {'class': 'pl'}):
        if ('åˆ¶ç‰‡å›½å®¶/åœ°åŒº:' == span.string):
            country = span.next_sibling.string
        if ('IMDb:' == span.string):
            imdb = 'https://www.imdb.com/title/'+span.next_sibling.string.strip()
    insert_movie(title, date, link, cover, rating, note, status,
                 year, directors, actors, genre, country, imdb)


def parse_book(date, rating, note, status, link):
    f = {"property": "æ¡ç›®é“¾æ¥", "url": {"equals": link}}
    response = notion_api.query_database(
        database_id=BOOK_DATABASE_ID, filter=f)
    if (len(response['results']) > 0):
        update(date, rating, note, status,response['results'][0]['id'])
        return
    response = requests.get(link, headers=headers)
    soup = BeautifulSoup(response.content)
    title = soup.find(property='v:itemreviewed').string
    #
    info = soup.find(id='info')
    info = list(map(lambda x: x.replace(':', '').strip(), list(
        filter(lambda x: '\n' not in x, info.strings))))
    dict = {}
    dict['ä½œè€…']=info[info.index('ä½œè€…')+1:info.index('å‡ºç‰ˆç¤¾')]
    dict['å‡ºç‰ˆå¹´']=info[info.index('å‡ºç‰ˆå¹´')+1:info.index('å‡ºç‰ˆå¹´')+2]
    dict['ISBN']=info[info.index('ISBN')+1:]
    cover = soup.find(id='mainpic').img['src']
    weread = search_book(title,dict['ISBN'][0])
    insert_book(title, date, link, cover, dict, rating, note, status,weread)

def search_book(keyword,ISBN):
    """æœç´¢ä¹¦ç±"""
    session.get(WEREAD_BASE_URL)
    id = ""
    url = "https://i.weread.qq.com/store/search"
    params = {"count": 10, "keyword": keyword}
    r = session.get(url, params=params)
    print(f"æœç´¢{keyword} ç»“æœ{r.ok}")
    for book in r.json()["books"]:
        bookId = book["bookInfo"]["bookId"]
        isbn = get_bookinfo(bookId=bookId)
        if isbn == ISBN:
            id = bookId
            break
    return id



def get_bookinfo(bookId):
    """è·å–ä¹¦çš„è¯¦æƒ…"""
    url = "https://i.weread.qq.com/book/info"
    params = dict(bookId=bookId)
    r = session.get(url, params=params)
    isbn = ""
    if r.ok:
        data = r.json()
        isbn = data["isbn"]
        title = data["title"]
        print(f"ä¹¦å{title} ISBN{isbn}")
    return isbn

def update(date,rating,note, status,page_id):
    properties = (
        Properties()
        .date(property='æ‰“åˆ†æ—¥æœŸ', start=date)
        .select('çŠ¶æ€', status)
    )
    properties = notion_api.get_relation(properties=properties,date=date)
    if rating != "":
        properties.select("ä¸ªäººè¯„åˆ†", rating)
    if note != "":
        properties.rich_text("æˆ‘çš„çŸ­è¯„", note)
    notion_api.update_page(page_id=page_id,properties=properties)


def insert_movie(title, date, link, cover, rating, note, status, year, directors, actors, genre, country, imdb):
    properties = (
        Properties()
        .title(title)
        .date(property='æ‰“åˆ†æ—¥æœŸ', start=date)
        .file("æµ·æŠ¥", cover)
        .url("æ¡ç›®é“¾æ¥", link)
        .number('ä¸Šæ˜ å¹´ä»½', int(year))
        .select('çŠ¶æ€', status)
        # .multi_select('å¯¼æ¼”', directors)
        # .multi_select('ä¸»æ¼”', actors[0:10])
        .multi_select('ç±»å‹', genre)
        .rich_text('åˆ¶ç‰‡å›½å®¶', country)
       
    )
    properties = notion_api.get_relation(properties=properties,date=date)
    if imdb!="":
         properties.url("IMDb é“¾æ¥", imdb)
    if rating != "":
        properties.select("ä¸ªäººè¯„åˆ†", rating)
    if note != "":
        properties.rich_text("æˆ‘çš„çŸ­è¯„", note)
    page = (
        Page()
        .parent(DatabaseParent(MOVIE_DATABASE_ID))
        .cover(cover)
        .icon("ğŸ¬")
        .children(Children())
        .properties(properties)
    )
    notion_api.create_page(page)
    print("æ’å…¥ "+title+" æˆåŠŸ")

#æ’å…¥
def insert_book(title, date, link, cover, info, rating, note, status,weread):
    s = info['å‡ºç‰ˆå¹´'][0]
    l = list(map(int, s.split('-')))
    l.append(1)
    properties = (
        Properties()
        .title(title)
        .date(property='æ‰“åˆ†æ—¥æœŸ', start=date)
        .file("æµ·æŠ¥", cover)
        .url("æ¡ç›®é“¾æ¥", link)
        .date(property='å‡ºç‰ˆæ—¥æœŸ', start=datetime(*l))
        .multi_select('ä½œè€…', info['ä½œè€…'])
        .number('ISBN', int(info['ISBN'][0]))
        .select('çŠ¶æ€', status)
    )
    if weread != "":
        properties.rich_text("WeRead", weread)
    properties = notion_api.get_relation(properties=properties,date=date)
    if rating != "":
        properties.select("ä¸ªäººè¯„åˆ†", rating)
    if note != "":
        properties.rich_text("æˆ‘çš„çŸ­è¯„", note)

    page = (
        Page()
        .parent(DatabaseParent(BOOK_DATABASE_ID))
        .cover(cover)
        .icon("ğŸ“š")
        .children(Children())
        .properties(properties)
    )
    notion_api.create_page(page)
    print("æ’å…¥ "+title+" æˆåŠŸ")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("cookies")
    options = parser.parse_args()
    cookies = options.cookies
    session = requests.Session()
    session.cookies = parse_cookie_string(cookies)    
    feed_parser()